#!/usr/bin/env python3
"""
Comprehensive Test for Enhanced Out-of-Dataset URL Prediction
"""

from enhanced_classifier import EnhancedURLClassifier
import time

def test_enhanced_out_of_dataset():
    """Test the enhanced classifier's out-of-dataset capabilities"""
    
    print("ðŸš€ TESTING ENHANCED OUT-OF-DATASET URL PREDICTION")
    print("=" * 75)
    
    # Load enhanced classifier
    classifier = EnhancedURLClassifier()
    classifier.load_model('models')
    print("âœ… Enhanced classifier loaded\n")
    
    # Comprehensive test cases
    test_categories = {
        "ðŸ”´ ADVANCED PHISHING (should be HIGH RISK)": [
            ("https://paypai-secure.tk", "Typosquatting + suspicious TLD"),
            ("https://goog1e-accounts.ml", "Character substitution + suspicious TLD"),  
            ("https://microsoft-security-center.ga", "Brand spoofing + suspicious TLD"),
            ("https://amazom-billing-department.cf", "Typosquatting + phishing keywords"),
            ("https://apple-id-verification-portal.pw", "Multiple phishing indicators"),
            ("https://facebook-account-suspended.click", "Social media phishing"),
            ("https://github-security-alert.download", "Developer platform spoofing"),
            ("https://linkedin-profile-locked.top", "Professional network phishing"),
        ],
        
        "ðŸŸ  CRYPTO & MODERN SCAMS (should be HIGH RISK)": [
            ("https://free-bitcoin-generator.racing", "Crypto scam pattern"),
            ("https://nft-airdrop-claim.stream", "NFT scam"),
            ("https://urgent-crypto-verification.review", "Crypto + urgency keywords"),
            ("https://blockchain-investment-bonus.science", "Investment scam"),
            ("https://win-ethereum-prize.party", "Prize scam + crypto"),
        ],
        
        "âš« MALWARE & EXPLOITS (should be HIGH RISK)": [
            ("http://192.168.1.100/payload.exe", "Direct IP + malware file"),
            ("https://download-cracked-software.work", "Software piracy"),
            ("https://free-game-hack-tools.download", "Gaming exploit"),
            ("https://crack-premium-software.tk", "Cracking tools"),
        ],
        
        "ðŸŸ¢ LEGITIMATE NEW SERVICES (should be LOW RISK)": [
            ("https://newtech-startup.com/about", "Tech startup"),
            ("https://www.localrestaurant.org/menu", "Local business"),
            ("https://university-research.edu/paper", "Educational content"),
            ("https://api.weatherdata.gov/forecast", "Government API"),
            ("https://cdn.moderntechframework.io/v2/lib.js", "Modern framework CDN"),
            ("https://docs.opensourceproject.org/guide", "Open source docs"),
            ("https://blog.techcompany.de/article", "German tech blog"),
            ("https://support.saasplatform.co.uk/help", "SaaS support"),
        ],
        
        "ðŸ”µ EDGE CASES & TRICKY PATTERNS": [
            ("https://bit.ly/cryptolink123", "URL shortener"),
            ("https://very-long-suspicious-domain-name.com", "Overly long domain"),
            ("https://test12345678.tk", "Random + suspicious TLD"),
            ("https://localhost:3000/development", "Local development"),
            ("https://secure-banking-system.online", "Suspicious keywords + generic TLD"),
        ]
    }
    
    # Run tests
    total_tests = 0
    results_by_category = {}
    
    for category, test_cases in test_categories.items():
        print(f"{category}")
        print("-" * 60)
        
        category_results = []
        for url, description in test_cases:
            total_tests += 1
            
            try:
                start_time = time.time()
                result = classifier.predict_url(url)
                prediction_time = time.time() - start_time
                
                prediction = result['prediction']
                confidence = result['confidence']
                reason = result['reason']
                
                # Determine risk level and display
                if prediction in ['phishing', 'malware', 'defacement']:
                    risk_display = "ðŸ”´ HIGH RISK"
                    risk_color = "red"
                elif prediction == 'benign':
                    risk_display = "ðŸŸ¢ LOW RISK"
                    risk_color = "green"
                else:
                    risk_display = "â“ UNKNOWN"
                    risk_color = "yellow"
                
                print(f"  URL: {url}")
                print(f"  Description: {description}")
                print(f"  Result: {risk_display} | {prediction} (confidence: {confidence:.3f})")
                print(f"  Reason: {reason}")
                print(f"  Speed: {prediction_time*1000:.1f}ms")
                print()
                
                category_results.append({
                    'url': url,
                    'prediction': prediction,
                    'confidence': confidence,
                    'reason': reason,
                    'risk_color': risk_color,
                    'description': description
                })
                
            except Exception as e:
                print(f"  âŒ ERROR: {e}")
                print()
        
        results_by_category[category] = category_results
    
    # Analysis
    print("=" * 75)
    print("ðŸ“Š ANALYSIS OF ENHANCED OUT-OF-DATASET PERFORMANCE")
    print("=" * 75)
    
    # Overall statistics
    all_predictions = []
    for results in results_by_category.values():
        all_predictions.extend(results)
    
    pred_counts = {}
    confidence_sum = 0
    for result in all_predictions:
        pred = result['prediction']
        pred_counts[pred] = pred_counts.get(pred, 0) + 1
        confidence_sum += result['confidence']
    
    avg_confidence = confidence_sum / len(all_predictions) if all_predictions else 0
    
    print(f"Total URLs tested: {len(all_predictions)}")
    print(f"Average confidence: {avg_confidence:.3f}")
    print("\nPrediction distribution:")
    for pred, count in sorted(pred_counts.items()):
        percentage = (count / len(all_predictions)) * 100
        print(f"  {pred}: {count} URLs ({percentage:.1f}%)")
    
    # Reasoning analysis
    reasons = {}
    for result in all_predictions:
        reason_key = result['reason'].split(':')[0].split('(')[0].strip()  # Extract main reason
        reasons[reason_key] = reasons.get(reason_key, 0) + 1
    
    print(f"\nReasoning methods used:")
    for reason, count in sorted(reasons.items(), key=lambda x: x[1], reverse=True):
        print(f"  {reason}: {count} URLs")
    
    # Performance assessment
    high_conf_predictions = len([r for r in all_predictions if r['confidence'] > 0.8])
    moderate_conf = len([r for r in all_predictions if 0.6 <= r['confidence'] <= 0.8])
    low_conf = len([r for r in all_predictions if r['confidence'] < 0.6])
    
    print(f"\nConfidence distribution:")
    print(f"  High confidence (>0.8): {high_conf_predictions} ({high_conf_predictions/len(all_predictions)*100:.1f}%)")
    print(f"  Moderate confidence (0.6-0.8): {moderate_conf} ({moderate_conf/len(all_predictions)*100:.1f}%)")
    print(f"  Low confidence (<0.6): {low_conf} ({low_conf/len(all_predictions)*100:.1f}%)")
    
    return results_by_category

def generate_streamlit_test_urls(results):
    """Generate URLs for testing in Streamlit app"""
    print("\nðŸŒ STREAMLIT APP TEST URLS")
    print("=" * 75)
    
    print("Copy these URLs to test in your Streamlit app:")
    print("(Should show same results as above)")
    print()
    
    # Extract a few representative URLs from each category
    test_urls = {
        "PHISHING (should be RED/HIGH RISK)": [
            "paypai-secure.tk",
            "goog1e-accounts.ml", 
            "microsoft-security-center.ga"
        ],
        "LEGITIMATE (should be GREEN/LOW RISK)": [
            "newtech-startup.com",
            "university-research.edu",
            "api.weatherdata.gov"
        ],
        "EDGE CASES (mixed results)": [
            "bit.ly/cryptolink123",
            "very-long-suspicious-domain-name.com",
            "localhost:3000"
        ]
    }
    
    for category, urls in test_urls.items():
        print(f"{category}:")
        for url in urls:
            print(f"  {url}")
        print()
    
    print("ðŸŽ¯ TESTING INSTRUCTIONS:")
    print("1. Open your Streamlit app (local or cloud)")
    print("2. Ensure 'Force Enhanced Classifier' is checked")
    print("3. Enable 'Debug Mode' for detailed output")
    print("4. Test each URL and compare results")
    print("5. Results should match the analysis above!")

if __name__ == "__main__":
    results = test_enhanced_out_of_dataset()
    generate_streamlit_test_urls(results)
    
    print("\nðŸŽ‰ ENHANCED CLASSIFIER CAPABILITIES SUMMARY:")
    print("âœ… Advanced typosquatting detection with character substitution")
    print("âœ… Modern threat pattern recognition (crypto scams, etc.)")
    print("âœ… Suspicious TLD and domain structure analysis") 
    print("âœ… Legitimate business pattern identification")
    print("âœ… Conservative approach for unknown/edge cases")
    print("âœ… Fast prediction speed with detailed reasoning")
    print("\nðŸš€ Ready for real-world deployment with out-of-dataset URLs!")